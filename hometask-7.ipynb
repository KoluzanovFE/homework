{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classwork: https://colab.research.google.com/drive/1Im7guDVqq5SK0UX6w4bc_ZUsLHyAsDeB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем словарь на основе нашего набора текстов. Для этого используем модуль CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.09, max_features=None, min_df=8,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'however', 'became', 'anywhere', 'an', 'both', 'am', 'anyhow', 'since', 'someone', 'is', 'almost', 'ten', 'as', 'in', 'they', 'hence', 'name', 'much', 'yourself', 'hereby', 'last', 'least', 'thereupon', 'get', 'whereas', 'any', 'move', 'now', 'seems', 'our', 'put', 'can', 'too'...her', 'perhaps', 'whoever', 'bill', 'hers', 'detail', 'herself', 'among', 'into', 'take', 'nobody'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "n = 854\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True,min_df = 8,max_df = 0.09)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведем счетчики n_d_k, n_k_w, n_k из нулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20) (20, 12424)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "x,y = X_train.shape\n",
    "K = 20\n",
    "len_dict = len(vectorizer.vocabulary_)\n",
    "Dict = vectorizer.vocabulary_\n",
    "n_d_k = np.zeros(shape = (x,K))\n",
    "n_k_w = np.zeros(shape = (K,len_dict))\n",
    "n_k = np.zeros(shape = K)\n",
    "columns = X_train.nonzero()[1]\n",
    "rows = X_train.nonzero()[0]\n",
    "non_zero_len= X_train.data.shape[0]\n",
    "print(n_d_k.shape,n_k_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим значения  в X_train.data и счетчиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.data = np.random.randint(low =1 ,high = 21,size = (non_zero_len,))\n",
    "for i in range(non_zero_len):\n",
    "    n_k[X_train.data[i]-1] += 1\n",
    "    n_k_w[X_train.data[i]-1][columns[i]] += 1\n",
    "    n_d_k[rows[i]][X_train.data[i]-1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем алгоритм случайного блуждания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(niter,X_train,n_k,n_k_w,n_d_k,K):\n",
    "    \n",
    "    non_zero_len= X_train.data.shape[0]\n",
    "    columns = X_train.nonzero()[1]\n",
    "    rows = X_train.nonzero()[0]\n",
    "    non_zero = X_train.data\n",
    "    \n",
    "    a = np.ones(K)\n",
    "    b = np.ones(X_train.shape[1])\n",
    "    b_summ = b.sum()\n",
    "    num = list(np.arange(0,K)+1)\n",
    "    \n",
    "    for j in range(niter):\n",
    "        for i in range(non_zero_len):\n",
    "                n_k[non_zero[i]-1] -= 1\n",
    "                n_k_w[non_zero[i]-1][columns[i]] -= 1\n",
    "                n_d_k[rows[i]][non_zero[i]-1] -= 1\n",
    "                \n",
    "                probability = np.array([(n_d_k[rows[i]][c]+a[c])*(n_k_w[c][columns[i]]+b[columns[i]])/(n_k[c]+b_summ)\n",
    "                                                                                                        for c in range(K)])\n",
    "                non_zero[i] = int(random.choices(num, weights =  probability/probability.sum())[0])\n",
    "                \n",
    "                n_k[non_zero[i]-1] += 1\n",
    "                n_k_w[non_zero[i]-1][columns[i]] += 1\n",
    "                n_d_k[rows[i]][non_zero[i]-1] += 1\n",
    "    return non_zero,n_k,n_k_w,n_d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rezult,_,n_k_w,_ = walk(50,X_train,n_k,n_k_w,n_d_k,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximus = np.zeros(shape = (K,10),dtype = int)\n",
    "for ni in range(10):\n",
    "    maxis = np.zeros(K,dtype = int)\n",
    "    for i in range(len_dict):\n",
    "        maxis = [i if n_k_w[j][i]>n_k_w[j][maxis[j]] else maxis[j] for j in range(K)]\n",
    "    for j in range(K):\n",
    "        maximus[j][ni] = maxis[j]\n",
    "        n_k_w[j][maxis[j]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme 0\t\tcs edu ideas looking posting process remember saying seen sorry\n",
      "Theme 1\t\tbest game games league play players season team win year\n",
      "Theme 2\t\tcouple course different edu far heard luck number tom yes\n",
      "Theme 3\t\tanybody appreciated edu knows list newsgroup post read sure thing\n",
      "Theme 4\t\tbetter didn going got ll maybe said sure thing things\n",
      "Theme 5\t\tbanks cause disease edu gordon intellect medical pitt soon surrender\n",
      "Theme 6\t\t10 11 12 14 16 17 18 19 25 45\n",
      "Theme 7\t\t10 current high hot light line low power radio using\n",
      "Theme 8\t\tcase control crime government gun guns law laws rights state\n",
      "Theme 9\t\tbike buy car cars engine friend looking miles nice road\n",
      "Theme 10\t\tchildren history israel israeli jews killed state war world years\n",
      "Theme 11\t\tadvance anybody appreciate edu got hi looking mail post sorry\n",
      "Theme 12\t\tal bob com dave internet look phone question thank yes\n",
      "Theme 13\t\tcode file files ftp help program set using window windows\n",
      "Theme 14\t\tbelieve edu heard interested kind post questions regards seen tell\n",
      "Theme 15\t\t1993 20 address date edu information nasa research space university\n",
      "Theme 16\t\tchip clipper encryption government information key keys phone public using\n",
      "Theme 17\t\tamerican clinton house money president program states support white year\n",
      "Theme 18\t\tcard computer disk drive hard mail memory price software video\n",
      "Theme 19\t\tbelieve bible christian fact god jesus life point question true\n"
     ]
    }
   ],
   "source": [
    "for i in range(K):\n",
    "    dat = np.zeros((1, len_dict))\n",
    "    for word in maximus[i]:\n",
    "        dat[0, word] = 1\n",
    "    print('Theme {}\\t\\t{}'.format(i,' '.join(vectorizer.inverse_transform(dat)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать выводы о содержании глав:\n",
    "\n",
    "0) Что-то про извинения\n",
    "\n",
    "1) Тема про командные игры\n",
    "\n",
    "2) Трудно определить\n",
    "\n",
    "3) Тема связанная со знаниями и чтением \n",
    "\n",
    "4) Трудно определить\n",
    "\n",
    "5) Что-то с общественными местами\n",
    "\n",
    "6) Цифры\n",
    "\n",
    "7) Что-то про радио\n",
    "\n",
    "8) Про политику  и оружее\n",
    "\n",
    "9) Про колесный транспорт\n",
    "\n",
    "10) Про войну в Израиле\n",
    "\n",
    "11) Трудно определить\n",
    "\n",
    "12) Трудно определить\n",
    "\n",
    "13) Написание программ в  Windows\n",
    "\n",
    "14) Трудно определить\n",
    "\n",
    "15) Космические исследования 1993 \n",
    "\n",
    "16) Прослушивание телефонов правительством\n",
    "\n",
    "17) Программа Клинтона\n",
    "\n",
    "18) Устройство компьютера\n",
    "\n",
    "19) Христианство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство догадок не совпали"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
